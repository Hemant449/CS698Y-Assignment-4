# -*- coding: utf-8 -*-
"""model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gnTBoURyF9QpxC8zdFo90OurwJrxVNEl
"""

import pandas as pd
import plotly.express as px
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, precision_score,recall_score

from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import AdaBoostClassifier
from xgboost import XGBClassifier
from sklearn import svm

from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.ensemble import VotingClassifier

from google.colab import files
uploaded=files.upload()

df = pd.read_csv("data.csv", sep=";")
df.head()

df.dtypes

df.info()

df['Target'].unique()

df['Target'].value_counts()

"""# **BIAS ANALYSIS**"""

# Identify potential protected attributes
protected_attributes = [
    'Marital status',
    'Nacionality',
    'Gender',
    'Scholarship holder',
    'Displaced',
    'Debtor',
    'Age at enrollment'
]

# Analyze the distribution of the target variable across different categories of each protected attribute.
for attr in protected_attributes:
    print(f"\n=== Subgroup report for {attr} ===")

    # Calculate count and representation
    subgroup_counts = df[attr].value_counts()
    subgroup_representation = df[attr].value_counts(normalize=True).map('{:.4f}'.format) # Format to 4 decimal places

    # Calculate dropout rate (assuming 'Dropout' is the unfavorable outcome)
    # We need to filter for the 'Dropout' target and then group by the protected attribute
    dropout_rates = df[df['Target'] == 'Dropout'][attr].value_counts() / subgroup_counts
    dropout_rates = dropout_rates.fillna(0).map('{:.4f}'.format) # Fill NaN with 0 and format

    # Create a DataFrame to display the report
    subgroup_report = pd.DataFrame({
        'count': subgroup_counts,
        'representation': subgroup_representation,
        'Dropout_rate': dropout_rates # Renamed to Dropout_rate for clarity
    })

    # Sort by count or representation for better readability if needed
    subgroup_report = subgroup_report.sort_values(by='count', ascending=False)

    display(subgroup_report)

    # Create 'AgeBin' column based on 'Age at enrollment'
def age_to_bin(age):
    if age <= 20:
        return '<=20'
    elif 21 <= age <= 25:
        return '21-25'
    elif 26 <= age <= 30:
        return '26-30'
    else:
        return '>30'

df['AgeBin'] = df['Age at enrollment'].apply(age_to_bin)

# Analyze the distribution of the target variable across the 'AgeBin'
print("\n=== Subgroup report for AgeBin ===")

# Calculate count and representation for AgeBin
subgroup_counts_agebin = df['AgeBin'].value_counts()
subgroup_representation_agebin = df['AgeBin'].value_counts(normalize=True).map('{:.4f}'.format) # Format to 4 decimal places

# Calculate dropout rate for AgeBin
dropout_rates_agebin = df[df['Target'] == 'Dropout']['AgeBin'].value_counts() / subgroup_counts_agebin
dropout_rates_agebin = dropout_rates_agebin.fillna(0).map('{:.4f}'.format) # Fill NaN with 0 and format

# Create a DataFrame to display the report for AgeBin
subgroup_report_agebin = pd.DataFrame({
    'count': subgroup_counts_agebin,
    'representation': subgroup_representation_agebin,
    'Dropout_rate': dropout_rates_agebin
})

# Sort by count for better readability
subgroup_report_agebin = subgroup_report_agebin.sort_values(by='count', ascending=False)

display(subgroup_report_agebin)

print("Bias Analysis Summary:")

print("\nMarital Status:")
print("- Marital status 1 (presumably single) has a higher graduation rate (51.4%) compared to other statuses, especially 2 (presumably married) with a 39.1% graduation rate and 6 with a 16.7% graduation rate.")
print("- Marital status 2, 4, 5, and 6 show notably higher dropout rates compared to status 1.")

print("\nNacionality:")
print("- Nacionality 1 has a graduation rate of 49.9%.")
print("- Some nacionalities (2, 11, 13, 14, 108, 62, 25, 32) have very few data points, making it difficult to draw conclusions.")
print("- Nacionality 17, 109 show 100% dropout rate, but only have 1 data point each.")
print("- Nacionality 100 shows a 66.7% dropout rate with 3 data points.")
print("- Nacionality 48 shows a 63.6% dropout rate with 11 data points.")


print("\nGender:")
print("- Gender 0 has a significantly higher graduation rate (57.9%) compared to Gender 1 (35.2%).")
print("- Gender 1 has a higher dropout rate (45.1%) compared to Gender 0 (25.1%). This indicates a substantial bias related to gender.")

print("\nAge at enrollment (Binned):")
print("- The subgroup report for 'AgeBin' shows that younger students (<=20) have a significantly lower dropout rate (21.25%) compared to older age groups, especially those in the '26-30' bin (59.79%) and '>30' bin (53.63%). This indicates a bias related to age at enrollment.") # Updated summary


print("\nScholarship Holder:")
print("- The subgroup report for 'Scholarship holder' shows that scholarship holders (group 1) have a significantly lower dropout rate (12.19%) compared to non-scholarship holders (group 0) (38.71%). This indicates a bias related to scholarship status.") # Added

print("\nDisplaced:") # Added
print("- The subgroup report for 'Displaced' shows that displaced students (group 1) have a lower dropout rate (27.58%) compared to non-displaced students (group 0) (37.64%). This indicates a bias related to displaced status.") # Added

print("\nDebtor:") # Added
print("- The subgroup report for 'Debtor' shows that debtors (group 1) have a significantly higher dropout rate (62.03%) compared to non-debtors (group 0) (28.28%). This indicates a substantial bias related to debtor status.") # Added


print("\nOverall Summary of Biases:")
print("- Significant disparities exist in graduation and dropout rates based on Gender, Age at enrollment, Scholarship holder status, Displaced status, and Debtor status.") # Updated to reflect AgeBin and new attributes
print("- Marital Status also shows notable differences in outcomes.")
print("- Nacionality shows some disparities, but many categories have limited data, making it challenging to assess definitively.")

from sklearn.metrics import classification_report, accuracy_score
import pandas as pd

# Define the 'favorable outcome' as 'Graduate'
favorable_outcome = 'Graduate'

# Function to calculate Disparate Impact and Demographic Parity Difference
def calculate_bias_metrics(df, protected_attribute, favorable_outcome):
    """Calculates Disparate Impact and Demographic Parity Difference."""
    # Get the categories of the protected attribute
    groups = df[protected_attribute].unique()
    if len(groups) < 2:
        print(f"Not enough groups in '{protected_attribute}' to calculate bias metrics.")
        return

    # Calculate the rate of favorable outcome for each group
    outcome_rates = df.groupby(protected_attribute)['Target'].apply(
        lambda x: (x == favorable_outcome).mean()
    )

    print(f"\nBias Metrics for: {protected_attribute}")
    print(f"Favorable outcome rate for each group:\n{outcome_rates}")

    # Calculate Disparate Impact (ratio of favorable outcomes)
    # Choose a base group (e.g., the group with the highest favorable outcome rate)
    # This provides a ratio >= 1 if there is bias against other groups
    # If all rates are 0, Disparate Impact cannot be calculated.
    if outcome_rates.max() == 0:
         print(f"All groups in '{protected_attribute}' have a favorable outcome rate of 0, Disparate Impact cannot be calculated.")
         disparate_impact = None
    else:
        # Find the group with the maximum favorable outcome rate to use as the base
        base_group = outcome_rates.idxmax()
        base_rate = outcome_rates[base_group]
        disparate_impact = outcome_rates / base_rate
        print(f"Disparate Impact (relative to group {base_group} - highest rate):")
        display(disparate_impact)


    # Calculate Demographic Parity Difference (difference in favorable outcomes)
    # Choose the group with the highest rate as the base for consistency
    base_group_dpd = outcome_rates.idxmax() if outcome_rates.max() > 0 else groups[0] # Use first group if max rate is 0
    base_rate_dpd = outcome_rates[base_group_dpd]
    demographic_parity_difference = outcome_rates - base_rate_dpd
    print(f"Demographic Parity Difference (relative to group {base_group_dpd} - highest rate):")
    display(demographic_parity_difference)


# Calculate bias metrics for Gender
calculate_bias_metrics(df, 'Gender', favorable_outcome)

# Calculate bias metrics for Marital status (focus on the main categories with sufficient data)
# Filter for marital statuses with a reasonable number of samples for more reliable metrics
marital_status_counts = df['Marital status'].value_counts()
main_marital_statuses = marital_status_counts[marital_status_counts > 30].index.tolist() # Using a threshold of 30 for example

if len(main_marital_statuses) >= 2:
    df_filtered_marital = df[df['Marital status'].isin(main_marital_statuses)].copy()
    calculate_bias_metrics(df_filtered_marital, 'Marital status', favorable_outcome)
else:
    print("\nNot enough main marital statuses to calculate bias metrics.")

# Calculate bias metrics for AgeBin (using the binned age column)
# Ensure 'AgeBin' column exists before calculating metrics
if 'AgeBin' in df.columns:
    calculate_bias_metrics(df, 'AgeBin', favorable_outcome)
else:
    print("\n'AgeBin' column not found. Please create it before calculating fairness metrics for age.")

# Calculate bias metrics for Scholarship holder
calculate_bias_metrics(df, 'Scholarship holder', favorable_outcome) # Added

# Calculate bias metrics for Displaced
calculate_bias_metrics(df, 'Displaced', favorable_outcome) # Added

# Calculate bias metrics for Debtor
calculate_bias_metrics(df, 'Debtor', favorable_outcome) # Added

# Note: Nacionality still has many categories with small sample sizes, making direct Disparate Impact/Demographic Parity Difference
# calculation less informative.

print("\nQuantified Bias Analysis Summary:")

print("\nGender Bias:")
print("- Disparate Impact (relative to Gender 0 - highest rate): Gender 0 has a graduation rate 1.64 times higher than Gender 1. An ideal disparate impact ratio is between 0.8 and 1.25, so 1.64 indicates significant bias against Gender 1.")
print("- Demographic Parity Difference (relative to Gender 0 - highest rate): The graduation rate for Gender 1 is 22.7 percentage points lower than for Gender 0.")

print("\nMarital Status Bias:")
# Calculate mean graduation rates for specific marital statuses to use in the summary
marital_status_1_rate = df_filtered_marital[df_filtered_marital['Marital status'] == 1]['Target'].eq('Graduate').mean()
marital_status_2_rate = df_filtered_marital[df_filtered_marital['Marital status'] == 2]['Target'].eq('Graduate').mean()
marital_status_4_rate = df_filtered_marital[df_filtered_marital['Marital status'] == 4]['Target'].eq('Graduate').mean()

# Calculate Disparate Impact and Demographic Parity Difference for Marital Status for summary
marital_status_2_di = marital_status_2_rate / marital_status_1_rate if marital_status_1_rate > 0 else float('inf')
marital_status_4_di = marital_status_4_rate / marital_status_1_rate if marital_status_1_rate > 0 else float('inf')
marital_status_2_dpd = (marital_status_2_rate - marital_status_1_rate) * 100
marital_status_4_dpd = (marital_status_4_rate - marital_status_1_rate) * 100

print("- Disparate Impact (relative to Marital status 1 - highest rate): Marital status 2 has a graduation rate {:.2f} times that of Marital status 1, and Marital status 4 has a graduation rate {:.2f} times that of Marital status 1. These ratios fall outside the 0.8-1.25 range, indicating potential bias against these groups.".format(marital_status_2_di, marital_status_4_di))
print("- Demographic Parity Difference (relative to Marital status 1 - highest rate): Marital status 2 has a graduation rate {:.2f} percentage points lower than Marital status 1, and Marital status 4 has a graduation rate {:.2f} percentage points lower than Marital status 1.".format(marital_status_2_dpd, marital_status_4_dpd))

print("\nAge at enrollment (Binned) Bias:")
print("- The 'AgeBin' analysis shows significant disparities.")
# Calculate mean graduation rates for specific age bins to use in the summary
agebin_le20_rate = df[df['AgeBin'] == '<=20']['Target'].eq('Graduate').mean()
agebin_21_25_rate = df[df['AgeBin'] == '21-25']['Target'].eq('Graduate').mean()
agebin_26_30_rate = df[df['AgeBin'] == '26-30']['Target'].eq('Graduate').mean()
agebin_gt30_rate = df[df['AgeBin'] == '>30']['Target'].eq('Graduate').mean()

# Calculate Disparate Impact and Demographic Parity Difference for AgeBin for summary
agebin_21_25_di = agebin_21_25_rate / agebin_le20_rate if agebin_le20_rate > 0 else float('inf')
agebin_26_30_di = agebin_26_30_rate / agebin_le20_rate if agebin_le20_rate > 0 else float('inf')
agebin_gt30_di = agebin_gt30_rate / agebin_le20_rate if agebin_le20_rate > 0 else float('inf')

agebin_21_25_dpd = (agebin_21_25_rate - agebin_le20_rate) * 100
agebin_26_30_dpd = (agebin_26_30_rate - agebin_le20_rate) * 100
agebin_gt30_dpd = (agebin_gt30_rate - agebin_le20_rate) * 100


print("- Disparate Impact (relative to AgeBin <=20 - highest rate):")
print("  - 21-25: {:.2f}".format(agebin_21_25_di))
print("  - 26-30: {:.2f}".format(agebin_26_30_di))
print("  - >30: {:.2f}".format(agebin_gt30_di))
print("  These ratios indicate significant bias against older age groups.")
print("- Demographic Parity Difference (relative to AgeBin <=20 - highest rate):")
print("  - 21-25: {:.2f} percentage points lower".format(agebin_21_25_dpd))
print("  - 26-30: {:.2f} percentage points lower".format(agebin_26_30_dpd))
print("  - >30: {:.2f} percentage points lower".format(agebin_gt30_dpd))


print("\nScholarship Holder Bias:") # Added
# Calculate mean graduation rates for Scholarship Holder to use in the summary
scholarship_0_rate = df[df['Scholarship holder'] == 0]['Target'].eq('Graduate').mean()
scholarship_1_rate = df[df['Scholarship holder'] == 1]['Target'].eq('Graduate').mean()

# Calculate Disparate Impact and Demographic Parity Difference for Scholarship Holder for summary
scholarship_di = scholarship_0_rate / scholarship_1_rate if scholarship_1_rate > 0 else float('inf')
scholarship_dpd = (scholarship_0_rate - scholarship_1_rate) * 100


print("- Disparate Impact (relative to group 1 - highest rate): Non-scholarship holders (group 0) have a graduation rate {:.2f} times that of scholarship holders (group 1). This ratio ({:.2f}) is outside the 0.8-1.25 range, indicating significant bias against non-scholarship holders.".format(scholarship_di, scholarship_di))
print("- Demographic Parity Difference (relative to group 1 - highest rate): The graduation rate for non-scholarship holders (group 0) is {:.2f} percentage points lower than for scholarship holders (group 1).".format(scholarship_dpd))

print("\nDisplaced Bias:") # Added
# Calculate mean graduation rates for Displaced to use in the summary
displaced_0_rate = df[df['Displaced'] == 0]['Target'].eq('Graduate').mean()
displaced_1_rate = df[df['Displaced'] == 1]['Target'].eq('Graduate').mean()

# Calculate Disparate Impact and Demographic Parity Difference for Displaced for summary
displaced_di = displaced_0_rate / displaced_1_rate if displaced_1_rate > 0 else float('inf')
displaced_dpd = (displaced_0_rate - displaced_1_rate) * 100

print("- Disparate Impact (relative to group 1 - highest rate): Non-displaced students (group 0) have a graduation rate {:.2f} times that of displaced students (group 1). This ratio ({:.2f}) is close to the ideal range, but still suggests some potential bias.".format(displaced_di, displaced_di))
print("- Demographic Parity Difference (relative to group 1 - highest rate): The graduation rate for non-displaced students (group 0) is {:.2f} percentage points lower than for displaced students (group 1).".format(displaced_dpd))

print("\nDebtor Bias:") # Added
# Calculate mean graduation rates for Debtor to use in the summary
debtor_0_rate = df[df['Debtor'] == 0]['Target'].eq('Graduate').mean()
debtor_1_rate = df[df['Debtor'] == 1]['Target'].eq('Graduate').mean()

# Calculate Disparate Impact and Demographic Parity Difference for Debtor for summary
debtor_di = debtor_1_rate / debtor_0_rate if debtor_0_rate > 0 else float('inf') # Relative to group 0 (highest rate)
debtor_dpd = (debtor_1_rate - debtor_0_rate) * 100 # Relative to group 0 (highest rate)

print("- Disparate Impact (relative to group 0 - highest rate): Debtors (group 1) have a graduation rate {:.2f} times that of non-debtors (group 0). This ratio ({:.2f}) is significantly outside the 0.8-1.25 range, indicating substantial bias against debtors.".format(debtor_di, debtor_di))
print("- Demographic Parity Difference (relative to group 0 - highest rate): The graduation rate for debtors (group 1) is {:.2f} percentage points lower than for non-debtors (group 0).".format(debtor_dpd))


print("\nOverall Summary of Biases:")
print("- The quantitative analysis confirms significant biases related to Gender, Age at enrollment (binned), Scholarship holder status, Displaced status, and Debtor status.")
print("- Gender and Debtor status show particularly large disparities in graduation rates.")
print("- Age at enrollment (binned) shows a trend of decreasing graduation rates with increasing age.")
print("- Marital Status also shows notable differences in outcomes.")
print("- Nacionality shows some disparities, but many categories have limited data, making it challenging to assess definitively.")

"""# Data Preprocessing"""

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import numpy as np
import pandas as pd # Import pandas

# Step 4: Separate features (X) and target (y)
X = df.drop('Target', axis=1)
y = df['Target']

# Convert target variable to numerical labels for modeling
# Map the target variable to numerical values
target_mapping = {'Graduate': 2, 'Enrolled': 1, 'Dropout': 0}
y = y.map(target_mapping)

# Step 2 & 3: Identify and preprocess features
# Identify categorical and numerical features
# Based on df.info(), these columns are currently int64 or object.
# We need to ensure 'AgeBin' is treated as categorical.

# Ensure 'AgeBin' is created before defining features if it's not already
if 'Age at enrollment' in df.columns and 'AgeBin' not in df.columns:
    def age_to_bin(age):
        if age <= 20:
            return '<=20'
        elif 21 <= age <= 25:
            return '21-25'
        elif 26 <= age <= 30:
            return '26-30'
        else:
            return '>30'
    df['AgeBin'] = df['Age at enrollment'].apply(age_to_bin)


categorical_features = [
    'Marital status',
    'Application mode',
    'Application order',
    'Course',
    "Daytime/evening attendance\t",
    'Previous qualification',
    'Nacionality',
    "Mother's qualification",
    "Father's qualification",
    "Mother's occupation",
    "Father's occupation",
    'Displaced',
    'Educational special needs',
    'Debtor',
    'Tuition fees up to date',
    'Gender',
    'Scholarship holder',
    'International',
    'AgeBin' # Added AgeBin to categorical features
]
numerical_features = X.select_dtypes(include=np.number).columns.tolist()

# Remove categorical features from numerical features list
numerical_features = [col for col in numerical_features if col not in categorical_features]
# Also remove 'Age at enrollment' from numerical features since 'AgeBin' is used
numerical_features = [col for col in numerical_features if col != 'Age at enrollment']


# Create preprocessing pipelines for numerical and categorical features
# Numerical features: scale using StandardScaler
numerical_transformer = StandardScaler()

# Categorical features: encode using OneHotEncoder
# Handle unknown categories by ignoring them during transform
categorical_transformer = OneHotEncoder(handle_unknown='ignore')

# Create a column transformer to apply different transformations to different columns
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numerical_transformer, numerical_features),
        ('cat', categorical_transformer, categorical_features)
    ],
    remainder='passthrough' # Keep other columns (if any) - though none expected here
)

# Apply the preprocessing to the features
X_processed = preprocessor.fit_transform(X)

# Step 5: Split data into training and testing sets
# We need the original data split before preprocessing to get X_test_orig for fairness evaluation later
X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)


# Apply preprocessing separately to train and test sets to avoid data leakage
X_train = preprocessor.transform(X_train_orig)
X_test = preprocessor.transform(X_test_orig)


# Ensure y_test_categorical is created for later use
inverse_target_mapping = {v: k for k, v in target_mapping.items()}
y_test_categorical = y_test_orig.map(inverse_target_mapping)


print("\nData preprocessing complete.")
print("Shape of original features (X):", X.shape)
print("Shape of processed features (X_processed):", X_processed.shape)
print("Shape of training features (X_train):", X_train.shape)
print("Shape of testing features (X_test):", X_test.shape)
print("Shape of training target (y_train):", y_train_orig.shape) # Corrected variable name
print("Shape of testing target (y_test):", y_test_orig.shape) # Corrected variable name

"""## Build initial predictive model"""

from sklearn.ensemble import RandomForestClassifier

# Instantiate a RandomForestClassifier model
model = RandomForestClassifier(random_state=42)

# Train the model using the training data
model.fit(X_train, y_train_orig)

print("Baseline model training complete.")

"""## Evaluate model fairness and performance


"""

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import pandas as pd

# Step 1: Make predictions on the test set X_test using the trained model.
y_pred = model.predict(X_test)

# Step 2: Calculate overall performance metrics (e.g., accuracy, classification report) on the test set.
accuracy = accuracy_score(y_test_orig, y_pred)
class_report = classification_report(y_test_orig, y_pred)

print("Overall Model Performance:")
print(f"Accuracy: {accuracy:.4f}")
print("\nClassification Report:")
print(class_report)

# Step 3: Identify the columns in the original X DataFrame that correspond to the protected attributes.
# Updated to use 'AgeBin' instead of 'Age at enrollment' for fairness evaluation
protected_attribute_cols = ['Gender', 'Marital status', 'AgeBin', 'Scholarship holder', 'Displaced', 'Debtor']


# Step 4: Map the numerical target values in y_test back to their original categorical labels.
# We need the inverse mapping of target_mapping
inverse_target_mapping = {v: k for k, v in target_mapping.items()}
y_test_categorical = y_test_orig.map(inverse_target_mapping)
y_pred_categorical = pd.Series(y_pred, index=y_test_orig.index).map(inverse_target_mapping) # Map predictions as well, ensuring index alignment


# Step 5: Create a DataFrame for fairness evaluation with original protected attributes and target labels.
# To do this accurately, we should use the original X_test_orig DataFrame which contains the original columns.
# This assumes X_test_orig was created correctly in the preprocessing step.

# Ensure X_test_orig is available and correctly split from the original data
if 'X_test_orig' not in locals():
    print("X_test_orig not found. Re-splitting the original data.")
    X_train_orig, X_test_orig, y_train_orig, y_test_orig = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )

# Create 'AgeBin' column in X_test_orig if it doesn't exist, needed for fairness_df
if 'Age at enrollment' in X_test_orig.columns and 'AgeBin' not in X_test_orig.columns:
    def age_to_bin(age):
        if age <= 20:
            return '<=20'
        elif 21 <= age <= 25:
            return '21-25'
        elif 26 <= age <= 30:
            return '26-30'
        else:
            return '>30'
    X_test_orig['AgeBin'] = X_test_orig['Age at enrollment'].apply(age_to_bin)


# Create a DataFrame for fairness evaluation with original protected attributes and target labels
# Explicitly select the protected attribute columns from X_test_orig
fairness_df = X_test_orig[protected_attribute_cols].copy()
fairness_df['True_Target'] = y_test_categorical.values
fairness_df['Predicted_Target'] = y_pred_categorical.values


# Define the favorable outcome
favorable_outcome = 'Graduate'
favorable_outcome_value = target_mapping[favorable_outcome] # Get the numerical value for the favorable outcome

# Step 6: For each protected attribute, calculate fairness metrics.
print("\nFairness Evaluation:")

def calculate_fairness_metrics_for_attribute(fairness_df, protected_attribute, favorable_outcome, favorable_outcome_value):
    """Calculates and prints Disparate Impact, Demographic Parity Difference, and Equal Opportunity."""
    print(f"\nAnalyzing fairness for: {protected_attribute}")

    # Filter out groups with very few samples in the test set to avoid unstable metrics
    # Using a threshold of 5 for example
    attr_counts_test = fairness_df[protected_attribute].value_counts()
    main_groups_test = attr_counts_test[attr_counts_test > 5].index.tolist()

    if len(main_groups_test) < 2:
        print(f"Not enough main groups in '{protected_attribute}' test set to calculate fairness metrics.")
        return

    fairness_df_filtered = fairness_df[fairness_df[protected_attribute].isin(main_groups_test)].copy()

    # --- Calculate Disparate Impact and Demographic Parity Difference ---
    # Calculate the rate of predicted favorable outcome for each group
    predicted_outcome_rates = fairness_df_filtered.groupby(protected_attribute)['Predicted_Target'].apply(
        lambda x: (x == favorable_outcome).mean()
    )

    display(f"Predicted favorable outcome rate ('{favorable_outcome}') for each group:\n{predicted_outcome_rates}")

    groups = predicted_outcome_rates.index.tolist()
    if len(groups) < 2:
        print(f"Not enough groups in '{protected_attribute}' to calculate bias metrics.")
        # Continue to Equal Opportunity calculation if possible
    else:
        # Calculate Disparate Impact (ratio of favorable outcomes)
        if predicted_outcome_rates.max() == 0:
             print(f"All groups in '{protected_attribute}' have a predicted favorable outcome rate of 0, Disparate Impact cannot be calculated.")
        else:
            base_group = predicted_outcome_rates.idxmax()
            base_rate = predicted_outcome_rates[base_group]
            if base_rate > 0:
                disparate_impact = predicted_outcome_rates / base_rate
                display(f"Disparate Impact (relative to group {base_group} - highest rate):")
                display(disparate_impact)
            else:
                 print(f"Base group '{base_group}' has a predicted favorable outcome rate of 0, Disparate Impact cannot be calculated.")


        # Calculate Demographic Parity Difference (difference in favorable outcomes)
        base_group_dpd = predicted_outcome_rates.idxmax() if predicted_outcome_rates.max() > 0 else groups[0]
        base_rate_dpd = predicted_outcome_rates[base_group_dpd]
        demographic_parity_difference = predicted_outcome_rates - base_rate_dpd
        display(f"Demographic Parity Difference (relative to group {base_group_dpd} - highest rate):")
        display(demographic_parity_difference)

    # --- Calculate Equal Opportunity ---
    print(f"\nCalculating Equal Opportunity for: {protected_attribute}")

    # Filter for instances where the true outcome is the favorable outcome
    favorable_outcome_df = fairness_df_filtered[fairness_df_filtered['True_Target'] == favorable_outcome].copy()

    if len(favorable_outcome_df) == 0:
        print(f"No instances with favorable outcome ('{favorable_outcome}') in the filtered test set for '{protected_attribute}'. Cannot calculate Equal Opportunity.")
        return

    # Calculate the True Positive Rate (TPR) for each group (rate of predicting favorable among true favorable)
    # Need to use the numerical target for confusion matrix calculation
    favorable_outcome_df['True_Target_Value'] = favorable_outcome_df['True_Target'].map(target_mapping)
    favorable_outcome_df['Predicted_Target_Value'] = favorable_outcome_df['Predicted_Target'].map(target_mapping)


    tpr_rates = favorable_outcome_df.groupby(protected_attribute).apply(
        lambda x: confusion_matrix(x['True_Target_Value'], x['Predicted_Target_Value'], labels=[0, 1, 2])[favorable_outcome_value, favorable_outcome_value] / (x['True_Target_Value'] == favorable_outcome_value).sum() if (x['True_Target_Value'] == favorable_outcome_value).sum() > 0 else 0
    )


    display(f"True Positive Rate ('{favorable_outcome}') for each group:\n{tpr_rates}")

    if len(tpr_rates) < 2:
        print(f"Not enough groups with favorable outcomes in '{protected_attribute}' to calculate Equal Opportunity Difference.")
        return

    # Calculate Equal Opportunity Difference (difference in TPRs)
    # Choose the group with the highest TPR as the base
    if tpr_rates.max() == 0:
         print(f"All groups in '{protected_attribute}' have a TPR of 0, Equal Opportunity Difference cannot be calculated.")
    else:
        base_group_eo = tpr_rates.idxmax()
        base_rate_eo = tpr_rates[base_group_eo]
        equal_opportunity_difference = tpr_rates - base_rate_eo
        display(f"Equal Opportunity Difference (relative to group {base_group_eo} - highest TPR):")
        display(equal_opportunity_difference)


# Calculate fairness metrics for each protected attribute
for attr in protected_attribute_cols:
    calculate_fairness_metrics_for_attribute(fairness_df, attr, favorable_outcome, favorable_outcome_value)

"""## Model Performance and Fairness Evaluation (Before Bias Mitigation)

This section summarizes the performance and fairness of the baseline Random Forest Classifier model on the test set.

### Overall Model Performance

Based on the classification report, the model achieved an overall accuracy of **{accuracy:.4f}**. The precision, recall, and F1-score for each class are as follows:
"""

print(class_report)

print("Predicted favorable outcome rate ('Graduate') for each group:\n", fairness_df.groupby('Gender')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()))
gender_di = fairness_df.groupby('Gender')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) / fairness_df.groupby('Gender')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDisparate Impact (relative to group {} - highest rate):".format(gender_di.idxmax()))
print(gender_di)
gender_dpd = fairness_df.groupby('Gender')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) - fairness_df.groupby('Gender')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDemographic Parity Difference (relative to group {} - highest rate):".format(gender_dpd.idxmax()))
print(gender_dpd)

true_graduates_gender = fairness_df[fairness_df['True_Target'] == 'Graduate']
gender_tpr = true_graduates_gender.groupby('Gender').apply(
    lambda x: (x['Predicted_Target'] == 'Graduate').sum() / len(x) if len(x) > 0 else 0
)
print("\nTrue Positive Rate ('Graduate') for each group:\n", gender_tpr)
gender_eo_diff = gender_tpr - gender_tpr.max()
print("\nEqual Opportunity Difference (relative to group {} - highest TPR):".format(gender_eo_diff.idxmax()))
print(gender_eo_diff)

# Filter for marital statuses with a reasonable number of samples in the test set
marital_status_counts_test = fairness_df['Marital status'].value_counts()
main_marital_statuses_test = marital_status_counts_test[marital_status_counts_test > 5].index.tolist()
fairness_df_filtered_marital_test = fairness_df[fairness_df['Marital status'].isin(main_marital_statuses_test)].copy()

print("Predicted favorable outcome rate ('Graduate') for each group:\n", fairness_df_filtered_marital_test.groupby('Marital status')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()))
marital_di = fairness_df_filtered_marital_test.groupby('Marital status')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) / fairness_df_filtered_marital_test.groupby('Marital status')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDisparate Impact (relative to group {} - highest rate):".format(marital_di.idxmax()))
print(marital_di)
marital_dpd = fairness_df_filtered_marital_test.groupby('Marital status')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) - fairness_df_filtered_marital_test.groupby('Marital status')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDemographic Parity Difference (relative to group {} - highest rate):".format(marital_dpd.idxmax()))
print(marital_dpd)

true_graduates_marital = fairness_df_filtered_marital_test[fairness_df_filtered_marital_test['True_Target'] == 'Graduate']
marital_tpr = true_graduates_marital.groupby('Marital status').apply(
    lambda x: (x['Predicted_Target'] == 'Graduate').sum() / len(x) if len(x) > 0 else 0
)
print("\nTrue Positive Rate ('Graduate') for each group:\n", marital_tpr)
marital_eo_diff = marital_tpr - marital_tpr.max()
print("\nEqual Opportunity Difference (relative to group {} - highest TPR):".format(marital_eo_diff.idxmax()))
print(marital_eo_diff)

print("Predicted favorable outcome rate ('Graduate') for each group:\n", fairness_df.groupby('AgeBin')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()))
agebin_di = fairness_df.groupby('AgeBin')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) / fairness_df.groupby('AgeBin')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDisparate Impact (relative to group {} - highest rate):".format(agebin_di.idxmax()))
print(agebin_di)
agebin_dpd = fairness_df.groupby('AgeBin')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) - fairness_df.groupby('AgeBin')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDemographic Parity Difference (relative to group {} - highest rate):".format(agebin_dpd.idxmax()))
print(agebin_dpd)

true_graduates_agebin = fairness_df[fairness_df['True_Target'] == 'Graduate']
agebin_tpr = true_graduates_agebin.groupby('AgeBin').apply(
    lambda x: (x['Predicted_Target'] == 'Graduate').sum() / len(x) if len(x) > 0 else 0
)
print("\nTrue Positive Rate ('Graduate') for each group:\n", agebin_tpr)
agebin_eo_diff = agebin_tpr - agebin_tpr.max()
print("\nEqual Opportunity Difference (relative to group {} - highest TPR):".format(agebin_eo_diff.idxmax()))
print(agebin_eo_diff)

print("Predicted favorable outcome rate ('Graduate') for each group:\n", fairness_df.groupby('Scholarship holder')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()))
scholarship_di = fairness_df.groupby('Scholarship holder')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) / fairness_df.groupby('Scholarship holder')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDisparate Impact (relative to group {} - highest rate):".format(scholarship_di.idxmax()))
print(scholarship_di)
scholarship_dpd = fairness_df.groupby('Scholarship holder')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) - fairness_df.groupby('Scholarship holder')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDemographic Parity Difference (relative to group {} - highest rate):".format(scholarship_dpd.idxmax()))
print(scholarship_dpd)

true_graduates_scholarship = fairness_df[fairness_df['True_Target'] == 'Graduate']
scholarship_tpr = true_graduates_scholarship.groupby('Scholarship holder').apply(
    lambda x: (x['Predicted_Target'] == 'Graduate').sum() / len(x) if len(x) > 0 else 0
)
print("\nTrue Positive Rate ('Graduate') for each group:\n", scholarship_tpr)
scholarship_eo_diff = scholarship_tpr - scholarship_tpr.max()
print("\nEqual Opportunity Difference (relative to group {} - highest TPR):".format(scholarship_eo_diff.idxmax()))
print(scholarship_eo_diff)

print("Predicted favorable outcome rate ('Graduate') for each group:\n", fairness_df.groupby('Displaced')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()))
displaced_di = fairness_df.groupby('Displaced')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) / fairness_df.groupby('Displaced')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDisparate Impact (relative to group {} - highest rate):".format(displaced_di.idxmax()))
print(displaced_di)
displaced_dpd = fairness_df.groupby('Displaced')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) - fairness_df.groupby('Displaced')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDemographic Parity Difference (relative to group {} - highest rate):".format(displaced_dpd.idxmax()))
print(displaced_dpd)

true_graduates_displaced = fairness_df[fairness_df['True_Target'] == 'Graduate']
displaced_tpr = true_graduates_displaced.groupby('Displaced').apply(
    lambda x: (x['Predicted_Target'] == 'Graduate').sum() / len(x) if len(x) > 0 else 0
)
print("\nTrue Positive Rate ('Graduate') for each group:\n", displaced_tpr)
displaced_eo_diff = displaced_tpr - displaced_tpr.max()
print("\nEqual Opportunity Difference (relative to group {} - highest TPR):".format(displaced_eo_diff.idxmax()))
print(displaced_eo_diff)

print("Predicted favorable outcome rate ('Graduate') for each group:\n", fairness_df.groupby('Debtor')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()))
debtor_di = fairness_df.groupby('Debtor')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) / fairness_df.groupby('Debtor')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDisparate Impact (relative to group {} - highest rate):".format(debtor_di.idxmax()))
print(debtor_di)
debtor_dpd = fairness_df.groupby('Debtor')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()) - fairness_df.groupby('Debtor')['Predicted_Target'].apply(lambda x: (x == 'Graduate').mean()).max()
print("\nDemographic Parity Difference (relative to group {} - highest rate):".format(debtor_dpd.idxmax()))
print(debtor_dpd)

true_graduates_debtor = fairness_df[fairness_df['True_Target'] == 'Graduate']
debtor_tpr = true_graduates_debtor.groupby('Debtor').apply(
    lambda x: (x['Predicted_Target'] == 'Graduate').sum() / len(x) if len(x) > 0 else 0
)
print("\nTrue Positive Rate ('Graduate') for each group:\n", debtor_tpr)
debtor_eo_diff = debtor_tpr - debtor_tpr.max()
print("\nEqual Opportunity Difference (relative to group {} - highest TPR):".format(debtor_eo_diff.idxmax()))
print(debtor_eo_diff)

"""# **Mitigating bias**


"""

# Train Multi-attribute Reweighed Model
protected_attribute_cols_eval = ['Gender', 'Marital status', 'AgeBin', 'Scholarship holder', 'Displaced', 'Debtor']
protected_attributes_reweigh_multi = ['Gender', 'Marital status', 'AgeBin', 'Scholarship holder', 'Displaced', 'Debtor']
train_df_reweigh_multi = pd.concat([X_train_orig, y_train_orig.rename('Target')], axis=1)
outcome_probabilities_multi = train_df_reweigh_multi['Target'].value_counts(normalize=True)
sample_weights_multi = np.ones(len(train_df_reweigh_multi))

grouped_data_multi = train_df_reweigh_multi.groupby(protected_attributes_reweigh_multi + ['Target'])

for group_keys, group_data in grouped_data_multi:
    protected_attribute_values = group_keys[:-1]
    outcome_value = group_keys[-1]
    p_intersection_given_outcome = len(group_data) / len(train_df_reweigh_multi[train_df_reweigh_multi['Target'] == outcome_value]) if len(train_df_reweigh_multi[train_df_reweigh_multi['Target'] == outcome_value]) > 0 else 0
    num_protected_groups = train_df_reweigh_multi.groupby(protected_attributes_reweigh_multi).ngroups
    target_p_intersection_given_outcome = 1.0 / num_protected_groups if num_protected_groups > 0 else 0

    if p_intersection_given_outcome > 0 and target_p_intersection_given_outcome > 0:
         weight = target_p_intersection_given_outcome / p_intersection_given_outcome
         mask = (train_df_reweigh_multi['Target'] == outcome_value)
         for i, attr in enumerate(protected_attributes_reweigh_multi):
             mask = mask & (train_df_reweigh_multi[attr] == protected_attribute_values[i])
         sample_weights_multi[mask] = weight
    else:
         mask = (train_df_reweigh_multi['Target'] == outcome_value)
         for i, attr in enumerate(protected_attributes_reweigh_multi):
             mask = mask & (train_df_reweigh_multi[attr] == protected_attribute_values[i])
         sample_weights_multi[mask] = 0.01 # Assign a small weight

sample_weights_multi = sample_weights_multi * (len(train_df_reweigh_multi) / sample_weights_multi.sum())

model_reweighed_multi = RandomForestClassifier(random_state=42)
model_reweighed_multi.fit(X_train, y_train_orig, sample_weight=sample_weights_multi)
y_pred_reweighed_multi = model_reweighed_multi.predict(X_test)
accuracy_reweighed_multi = accuracy_score(y_test_orig, y_pred_reweighed_multi)
class_report_reweighed_multi = classification_report(y_test_orig, y_pred_reweighed_multi)
print("Multi-attribute reweighed model trained and evaluated.")

# Create fairness_df for multi-attribute reweighed model
y_pred_reweighed_multi_categorical = pd.Series(y_pred_reweighed_multi, index=X_test_orig.index).map(inverse_target_mapping)
fairness_df_reweighed_multi = X_test_orig[protected_attribute_cols_eval].copy()
fairness_df_reweighed_multi['True_Target'] = y_test_categorical.values
fairness_df_reweighed_multi['Predicted_Target'] = y_pred_reweighed_multi_categorical.values

"""## Evaluate Mitigated Model Fairness and Performance

### Subtask:
Evaluate the multi-attribute reweighted model's performance using appropriate metrics (e.g., accuracy, precision, recall) and assess its fairness across different protected groups using fairness metrics (e.g., demographic parity, equalized odds, disparate impact).
"""

from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import pandas as pd

# Assuming y_pred_reweighed_multi contains the predictions from the mitigated model
# Assuming y_test_orig contains the true labels for the test set
# Assuming X_test_orig contains the original features for the test set
# Assuming target_mapping and inverse_target_mapping are defined

# Step 1: Make predictions on the test set X_test using the trained mitigated model.
# This step was already done when training the reweighed model, y_pred_reweighed_multi is available.
# y_pred_reweighed_multi = model_reweighed_multi.predict(X_test) # No need to re-predict

# Step 2: Calculate overall performance metrics (e.g., accuracy, classification report) on the test set.
accuracy_mitigated = accuracy_score(y_test_orig, y_pred_reweighed_multi)
class_report_mitigated = classification_report(y_test_orig, y_pred_reweighed_multi)

print("Overall Mitigated Model Performance:")
print(f"Accuracy: {accuracy_mitigated:.4f}")
print("\nClassification Report:")
print(class_report_mitigated)

# Step 3: Identify the columns in the original X DataFrame that correspond to the protected attributes.
# Updated to use 'AgeBin' instead of 'Age at enrollment' for fairness evaluation
protected_attribute_cols_eval = ['Gender', 'Marital status', 'AgeBin', 'Scholarship holder', 'Displaced', 'Debtor']

# Step 4: Map the numerical target values in y_test back to their original categorical labels.
# This was already done in the baseline evaluation, y_test_categorical is available.
# inverse_target_mapping = {v: k for k, v in target_mapping.items()}
# y_test_categorical = y_test_orig.map(inverse_target_mapping)
y_pred_reweighed_multi_categorical = pd.Series(y_pred_reweighed_multi, index=y_test_orig.index).map(inverse_target_mapping) # Map predictions as well, ensuring index alignment


# Step 5: Create a DataFrame for fairness evaluation with original protected attributes and target labels.
# To do this accurately, we should use the original X_test_orig DataFrame which contains the original columns.
# This assumes X_test_orig was created correctly in the preprocessing step.

# Ensure X_test_orig is available and correctly split from the original data
if 'X_test_orig' not in locals():
    print("X_test_orig not found. Please ensure the preprocessing step was run.")
    # You might need to re-run the preprocessing cell if X_test_orig is missing.
else:
    # Create 'AgeBin' column in X_test_orig if it doesn't exist, needed for fairness_df
    if 'Age at enrollment' in X_test_orig.columns and 'AgeBin' not in X_test_orig.columns:
        def age_to_bin(age):
            if age <= 20:
                return '<=20'
            elif 21 <= age <= 25:
                return '21-25'
            elif 26 <= age <= 30:
                return '26-30'
            else:
                return '>30'
        X_test_orig['AgeBin'] = X_test_orig['Age at enrollment'].apply(age_to_bin)


    # Create a DataFrame for fairness evaluation with original protected attributes and target labels
    # Explicitly select the protected attribute columns from X_test_orig
    fairness_df_mitigated = X_test_orig[protected_attribute_cols_eval].copy()
    fairness_df_mitigated['True_Target'] = y_test_categorical.values
    fairness_df_mitigated['Predicted_Target'] = y_pred_reweighed_multi_categorical.values


    # Define the favorable outcome
    favorable_outcome = 'Graduate'
    favorable_outcome_value = target_mapping[favorable_outcome] # Get the numerical value for the favorable outcome

    # Step 6: For each protected attribute, calculate fairness metrics.
    print("\nFairness Evaluation for Mitigated Model:")

    def calculate_fairness_metrics_for_attribute(fairness_df, protected_attribute, favorable_outcome, favorable_outcome_value, target_mapping, threshold=5):
        """Calculates and prints Disparate Impact, Demographic Parity Difference, and Equal Opportunity."""
        print(f"\nAnalyzing fairness for: {protected_attribute}")

        # Filter out groups with very few samples in the test set to avoid unstable metrics
        attr_counts_test = fairness_df[protected_attribute].value_counts()
        main_groups_test = attr_counts_test[attr_counts_test > threshold].index.tolist()

        if len(main_groups_test) < 2:
            print(f"Not enough main groups in '{protected_attribute}' test set (>{threshold} samples) to calculate fairness metrics.")
            return

        df_filtered = fairness_df[fairness_df[protected_attribute].isin(main_groups_test)].copy()


        # --- Calculate Disparate Impact and Demographic Parity Difference ---
        # Calculate the rate of predicted favorable outcome for each group
        predicted_outcome_rates = df_filtered.groupby(protected_attribute)['Predicted_Target'].apply(
            lambda x: (x == favorable_outcome).mean()
        )

        display(f"Predicted favorable outcome rate ('{favorable_outcome}') for each group:\n{predicted_outcome_rates}")

        groups = predicted_outcome_rates.index.tolist()
        if len(groups) < 2:
            print(f"Not enough groups in '{protected_attribute}' to calculate bias metrics.")
            # Continue to Equal Opportunity calculation if possible
        else:
            # Calculate Disparate Impact (ratio of favorable outcomes)
            if predicted_outcome_rates.max() == 0:
                 print(f"All groups in '{protected_attribute}' have a predicted favorable outcome rate of 0, Disparate Impact cannot be calculated.")
            else:
                base_group = predicted_outcome_rates.idxmax()
                base_rate = predicted_outcome_rates[base_group]
                if base_rate > 0:
                    disparate_impact = predicted_outcome_rates / base_rate
                    display(f"Disparate Impact (relative to group {base_group} - highest rate):")
                    display(disparate_impact)
                else:
                     print(f"Base group '{base_group}' has a predicted favorable outcome rate of 0, Disparate Impact cannot be calculated.")


            # Calculate Demographic Parity Difference (difference in favorable outcomes)
            base_group_dpd = predicted_outcome_rates.idxmax() if predicted_outcome_rates.max() > 0 else groups[0]
            base_rate_dpd = predicted_outcome_rates[base_group_dpd]
            demographic_parity_difference = predicted_outcome_rates - base_rate_dpd
            display(f"Demographic Parity Difference (relative to group {base_group_dpd} - highest rate):")
            display(demographic_parity_difference)

        # --- Calculate Equal Opportunity ---
        print(f"\nCalculating Equal Opportunity for: {protected_attribute}")

        # Filter for instances where the true outcome is the favorable outcome
        favorable_outcome_df = df_filtered[df_filtered['True_Target'] == favorable_outcome].copy()

        if len(favorable_outcome_df) == 0:
            print(f"No instances with favorable outcome ('{favorable_outcome}') in the filtered test set for '{protected_attribute}'. Cannot calculate Equal Opportunity.")
            return

        # Calculate the True Positive Rate (TPR) for each group (rate of predicting favorable among true favorable)
        # Need to use the numerical target for confusion matrix calculation
        favorable_outcome_df['True_Target_Value'] = favorable_outcome_df['True_Target'].map(target_mapping)
        favorable_outcome_df['Predicted_Target_Value'] = favorable_outcome_df['Predicted_Target'].map(target_mapping)

        # Handle cases where a group might not have any true favorable outcomes
        # Explicitly select grouping columns to avoid DeprecationWarning
        tpr_rates = favorable_outcome_df.groupby(protected_attribute)[['True_Target_Value', 'Predicted_Target_Value']].apply(
            lambda x: confusion_matrix(x['True_Target_Value'], x['Predicted_Target_Value'], labels=[0, 1, 2])[favorable_outcome_value, favorable_outcome_value] / (x['True_Target_Value'] == favorable_outcome_value).sum() if (x['True_Target_Value'] == favorable_outcome_value).sum() > 0 else 0
        )


        display(f"True Positive Rate ('{favorable_outcome}') for each group:\n{tpr_rates}")

        if len(tpr_rates) < 2:
            print(f"Not enough groups with favorable outcomes in '{protected_attribute}' to calculate Equal Opportunity Difference.")
            return

        # Calculate Equal Opportunity Difference (difference in TPRs)
        # Choose the group with the highest TPR as the base
        if tpr_rates.max() == 0:
             print(f"All groups in '{protected_attribute}' have a TPR of 0, Equal Opportunity Difference cannot be calculated.")
        else:
            base_group_eo = tpr_rates.idxmax()
            base_rate_eo = tpr_rates[base_group_eo]
            equal_opportunity_difference = tpr_rates - base_rate_eo
            display(f"Equal Opportunity Difference (relative to group {base_group_eo} - highest TPR):")
            display(equal_opportunity_difference)


    # Calculate fairness metrics for each protected attribute for the mitigated model
    for attr in protected_attribute_cols_eval:
        calculate_fairness_metrics_for_attribute(fairness_df_mitigated, attr, favorable_outcome, favorable_outcome_value, target_mapping)

"""## Mitigated Model Performance and Fairness Evaluation (After Bias Mitigation)

This section summarizes the performance and fairness of the multi-attribute reweighted Random Forest Classifier model on the test set.

### Overall Model Performance

Based on the classification report, the mitigated model achieved an overall accuracy of **{accuracy_mitigated:.4f}**. The precision, recall, and F1-score for each class are as follows:
"""

print(class_report_mitigated)

"""### Fairness Evaluation

Here are the fairness metrics for the multi-attribute reweighted model across the protected attributes:

**Gender:**
"""

# Calculate and display fairness metrics for Gender for the mitigated model
# Use the calculate_fairness_metrics_for_attribute function defined previously
calculate_fairness_metrics_for_attribute(fairness_df_mitigated, 'Gender', favorable_outcome, favorable_outcome_value, target_mapping)

"""**Marital Status:**"""

# Calculate and display fairness metrics for Marital status for the mitigated model
calculate_fairness_metrics_for_attribute(fairness_df_mitigated, 'Marital status', favorable_outcome, favorable_outcome_value, target_mapping)

"""**Age at enrollment (Binned):**"""

# Calculate and display fairness metrics for AgeBin for the mitigated model
calculate_fairness_metrics_for_attribute(fairness_df_mitigated, 'AgeBin', favorable_outcome, favorable_outcome_value, target_mapping)

"""**Scholarship Holder:**"""

# Calculate and display fairness metrics for Scholarship holder for the mitigated model
calculate_fairness_metrics_for_attribute(fairness_df_mitigated, 'Scholarship holder', favorable_outcome, favorable_outcome_value, target_mapping)

"""**Displaced:**"""

# Calculate and display fairness metrics for Displaced for the mitigated model
calculate_fairness_metrics_for_attribute(fairness_df_mitigated, 'Displaced', favorable_outcome, favorable_outcome_value, target_mapping)

"""**Debtor:**"""

# Calculate and display fairness metrics for Debtor for the mitigated model
calculate_fairness_metrics_for_attribute(fairness_df_mitigated, 'Debtor', favorable_outcome, favorable_outcome_value, target_mapping)

"""## Compare Results and Summarize


"""

# Assuming the following variables from previous steps are available:
# accuracy (baseline model)
# class_report (baseline model)
# fairness_df (baseline fairness DataFrame)
# accuracy_mitigated (mitigated model)
# class_report_mitigated (mitigated model)
# fairness_df_mitigated (mitigated fairness DataFrame)
# target_mapping
# favorable_outcome
# protected_attribute_cols_eval

print("\nComparison of Model Performance:")
print(f"Baseline Model Accuracy: {accuracy:.4f}")
print(f"Multi-attribute Reweighed Model Accuracy: {accuracy_mitigated:.4f}")

print("\nComparison of Classification Reports:")
print("\nBaseline Model Classification Report:")
print(class_report)

print("\nMulti-attribute Reweighed Model Classification Report:")
print(class_report_mitigated)

print("\nComparison of Fairness Metrics ('Graduate' as favorable outcome):")

def calculate_and_display_fairness_metrics_comparison(baseline_df, mitigated_df, protected_attribute, favorable_outcome, favorable_outcome_value, target_mapping, threshold=5):
    """Calculates and prints Disparate Impact, Demographic Parity Difference, and Equal Opportunity for comparison."""
    print(f"\nAnalyzing fairness for: {protected_attribute}")

    # Filter out groups with very few samples in the test set for both dataframes
    attr_counts_baseline = baseline_df[protected_attribute].value_counts()
    main_groups_baseline = attr_counts_baseline[attr_counts_baseline > threshold].index.tolist()

    attr_counts_mitigated = mitigated_df[protected_attribute].value_counts()
    main_groups_mitigated = attr_counts_mitigated[attr_counts_mitigated > threshold].index.tolist()

    # Use common groups for comparison
    common_groups = list(set(main_groups_baseline) & set(main_groups_mitigated))

    if len(common_groups) < 2:
        print(f"Not enough common main groups in '{protected_attribute}' test sets (>{threshold} samples) to calculate fairness metrics for comparison.")
        return None, None, None # Return None if not enough groups

    baseline_df_filtered = baseline_df[baseline_df[protected_attribute].isin(common_groups)].copy()
    mitigated_df_filtered = mitigated_df[mitigated_df[protected_attribute].isin(common_groups)].copy()


    # --- Calculate Disparate Impact and Demographic Parity Difference ---
    # Calculate the rate of predicted favorable outcome for each group
    predicted_outcome_rates_baseline = baseline_df_filtered.groupby(protected_attribute)['Predicted_Target'].apply(
        lambda x: (x == favorable_outcome).mean()
    )
    predicted_outcome_rates_mitigated = mitigated_df_filtered.groupby(protected_attribute)['Predicted_Target'].apply(
        lambda x: (x == favorable_outcome).mean()
    )

    display(f"Predicted favorable outcome rate ('{favorable_outcome}') for each group:")
    display("Baseline:\n", predicted_outcome_rates_baseline)
    display("Mitigated:\n", predicted_outcome_rates_mitigated)


    # Calculate Disparate Impact (ratio of favorable outcomes) - using baseline highest rate as reference
    if predicted_outcome_rates_baseline.max() == 0:
         print(f"All groups in '{protected_attribute}' baseline have a predicted favorable outcome rate of 0, Disparate Impact cannot be calculated.")
         disparate_impact_baseline = None
         disparate_impact_mitigated = None
    else:
        base_group_di_baseline = predicted_outcome_rates_baseline.idxmax()
        base_rate_di_baseline = predicted_outcome_rates_baseline[base_group_di_baseline]
        if base_rate_di_baseline > 0:
            disparate_impact_baseline = predicted_outcome_rates_baseline / base_rate_di_baseline
            disparate_impact_mitigated = predicted_outcome_rates_mitigated / base_rate_di_baseline
            display(f"Disparate Impact (relative to baseline group {base_group_di_baseline} - highest baseline rate):")
            display("Baseline:\n", disparate_impact_baseline)
            display("Mitigated:\n", disparate_impact_mitigated)
        else:
             print(f"Baseline base group '{base_group_di_baseline}' has a predicted favorable outcome rate of 0, Disparate Impact cannot be calculated.")
             disparate_impact_baseline = None
             disparate_impact_mitigated = None


    # Calculate Demographic Parity Difference (difference in favorable outcomes) - using baseline highest rate as reference
    base_group_dpd_baseline = predicted_outcome_rates_baseline.idxmax() if predicted_outcome_rates_baseline.max() > 0 else (common_groups[0] if common_groups else None)
    if base_group_dpd_baseline is not None:
        base_rate_dpd_baseline = predicted_outcome_rates_baseline[base_group_dpd_baseline]
        demographic_parity_difference_baseline = predicted_outcome_rates_baseline - base_rate_dpd_baseline
        demographic_parity_difference_mitigated = predicted_outcome_rates_mitigated - base_rate_dpd_baseline
        display(f"Demographic Parity Difference (relative to baseline group {base_group_dpd_baseline} - highest baseline rate):")
        display("Baseline:\n", demographic_parity_difference_baseline)
        display("Mitigated:\n", demographic_parity_difference_mitigated)
    else:
        print("Could not determine a base group for Demographic Parity Difference calculation.")
        demographic_parity_difference_baseline = None
        demographic_parity_difference_mitigated = None


    # --- Calculate Equal Opportunity ---
    print(f"\nCalculating Equal Opportunity for: {protected_attribute}")

    # Filter for instances where the true outcome is the favorable outcome
    favorable_outcome_df_baseline = baseline_df_filtered[baseline_df_filtered['True_Target'] == favorable_outcome].copy()
    favorable_outcome_df_mitigated = mitigated_df_filtered[mitigated_df_filtered['True_Target'] == favorable_outcome].copy()


    if len(favorable_outcome_df_baseline) == 0 or len(favorable_outcome_df_mitigated) == 0:
        print(f"Not enough instances with favorable outcome ('{favorable_outcome}') in the filtered test sets for '{protected_attribute}'. Cannot calculate Equal Opportunity.")
        tpr_rates_baseline = None
        tpr_rates_mitigated = None
        equal_opportunity_difference_baseline = None
        equal_opportunity_difference_mitigated = None
    else:
        # Calculate the True Positive Rate (TPR) for each group (rate of predicting favorable among true favorable)
        # Need to use the numerical target for confusion matrix calculation
        favorable_outcome_df_baseline['True_Target_Value'] = favorable_outcome_df_baseline['True_Target'].map(target_mapping)
        favorable_outcome_df_baseline['Predicted_Target_Value'] = favorable_outcome_df_baseline['Predicted_Target'].map(target_mapping)

        favorable_outcome_df_mitigated['True_Target_Value'] = favorable_outcome_df_mitigated['True_Target'].map(target_mapping)
        favorable_outcome_df_mitigated['Predicted_Target_Value'] = favorable_outcome_df_mitigated['Predicted_Target'].map(target_mapping)


        # Explicitly select grouping columns to avoid DeprecationWarning
        tpr_rates_baseline = favorable_outcome_df_baseline.groupby(protected_attribute)[['True_Target_Value', 'Predicted_Target_Value']].apply(
            lambda x: confusion_matrix(x['True_Target_Value'], x['Predicted_Target_Value'], labels=[0, 1, 2])[favorable_outcome_value, favorable_outcome_value] / (x['True_Target_Value'] == favorable_outcome_value).sum() if (x['True_Target_Value'] == favorable_outcome_value).sum() > 0 else 0
        )

        tpr_rates_mitigated = favorable_outcome_df_mitigated.groupby(protected_attribute)[['True_Target_Value', 'Predicted_Target_Value']].apply(
            lambda x: confusion_matrix(x['True_Target_Value'], x['Predicted_Target_Value'], labels=[0, 1, 2])[favorable_outcome_value, favorable_outcome_value] / (x['True_Target_Value'] == favorable_outcome_value).sum() if (x['True_Target_Value'] == favorable_outcome_value).sum() > 0 else 0
        )

        display(f"True Positive Rate ('{favorable_outcome}') for each group:")
        display("Baseline:\n", tpr_rates_baseline)
        display("Mitigated:\n", tpr_rates_mitigated)

        if len(tpr_rates_baseline) < 2 or len(tpr_rates_mitigated) < 2:
            print(f"Not enough groups with favorable outcomes in '{protected_attribute}' to calculate Equal Opportunity Difference.")
            equal_opportunity_difference_baseline = None
            equal_opportunity_difference_mitigated = None
        else:
            # Calculate Equal Opportunity Difference (difference in TPRs) - using baseline highest TPR as reference
            if tpr_rates_baseline.max() == 0:
                 print(f"All groups in '{protected_attribute}' baseline have a TPR of 0, Equal Opportunity Difference cannot be calculated.")
                 equal_opportunity_difference_baseline = None
                 equal_opportunity_difference_mitigated = None
            else:
                base_group_eo_baseline = tpr_rates_baseline.idxmax()
                base_rate_eo_baseline = tpr_rates_baseline[base_group_eo_baseline]
                equal_opportunity_difference_baseline = tpr_rates_baseline - base_rate_eo_baseline
                equal_opportunity_difference_mitigated = tpr_rates_mitigated - base_rate_eo_baseline
                display(f"Equal Opportunity Difference (relative to baseline group {base_group_eo_baseline} - highest baseline TPR):")
                display("Baseline:\n", equal_opportunity_difference_baseline)
                display("Mitigated:\n", equal_opportunity_difference_mitigated)

        return disparate_impact_baseline, disparate_impact_mitigated, demographic_parity_difference_baseline, demographic_parity_difference_mitigated, equal_opportunity_difference_baseline, equal_opportunity_difference_mitigated


# Store metrics for comparison
baseline_metrics = {}
mitigated_metrics = {}

favorable_outcome_label = 'Graduate'
favorable_outcome_value = target_mapping[favorable_outcome_label]

# Need to create fairness_df_baseline here as it was not explicitly created before the mitigated evaluation
if 'fairness_df' not in locals():
    print("fairness_df (baseline) not found. Re-creating for comparison.")
    # Assuming X_test_orig and y_test_categorical are available from preprocessing
    if 'X_test_orig' in locals() and 'y_test_categorical' in locals():
         # Ensure 'AgeBin' column exists in X_test_orig if it doesn't, needed for fairness_df
        if 'Age at enrollment' in X_test_orig.columns and 'AgeBin' not in X_test_orig.columns:
            def age_to_bin(age):
                if age <= 20:
                    return '<=20'
                elif 21 <= age <= 25:
                    return '21-25'
                elif 26 <= age <= 30:
                    return '26-30'
                else:
                    return '>30'
            X_test_orig['AgeBin'] = X_test_orig['Age at enrollment'].apply(age_to_bin)

        # Need baseline predictions to create fairness_df
        # Assuming 'model' (baseline model) and 'X_test' are available
        if 'model' in locals() and 'X_test' in locals():
            y_pred_baseline = model.predict(X_test)
            y_pred_baseline_categorical = pd.Series(y_pred_baseline, index=y_test_orig.index).map(inverse_target_mapping) # Ensure index alignment

            fairness_df_baseline = X_test_orig[protected_attribute_cols_eval].copy()
            fairness_df_baseline['True_Target'] = y_test_categorical.values
            fairness_df_baseline['Predicted_Target'] = y_pred_baseline_categorical.values
            print("fairness_df (baseline) created.")
        else:
            print("Could not create fairness_df (baseline). Please ensure baseline model and test data are available.")
            fairness_df_baseline = None # Set to None if cannot create
    else:
        print("Could not create fairness_df (baseline). Please ensure X_test_orig and y_test_categorical are available from preprocessing.")
        fairness_df_baseline = None # Set to None if cannot create
else:
     fairness_df_baseline = fairness_df.copy() # Use the existing fairness_df as baseline if available
     # Ensure 'Predicted_Target' is in fairness_df
     if 'Predicted_Target' not in fairness_df_baseline.columns:
         # Assuming 'model' (baseline model) and 'X_test' are available
         if 'model' in locals() and 'X_test' in locals():
             y_pred_baseline = model.predict(X_test)
             y_pred_baseline_categorical = pd.Series(y_pred_baseline, index=y_test_orig.index).map(inverse_target_mapping) # Ensure index alignment
             fairness_df_baseline['Predicted_Target'] = y_pred_baseline_categorical.values
             print("Added 'Predicted_Target' to existing fairness_df (baseline).")
         else:
             print("Could not add 'Predicted_Target' to existing fairness_df. Baseline predictions not available.")
             fairness_df_baseline = None # Set to None if cannot proceed

if fairness_df_baseline is not None and 'fairness_df_mitigated' in locals():
    for attr in protected_attribute_cols_eval:
        di_b, di_m, dpd_b, dpd_m, eo_b, eo_m = calculate_and_display_fairness_metrics_comparison(fairness_df_baseline, fairness_df_mitigated, attr, favorable_outcome_label, favorable_outcome_value, target_mapping)
        baseline_metrics[attr] = {'di': di_b, 'dpd': dpd_b, 'eo_diff': eo_b}
        mitigated_metrics[attr] = {'di': di_m, 'dpd': dpd_m, 'eo_diff': eo_m}
else:
    print("\nCould not perform fairness metrics comparison. Ensure both baseline and mitigated fairness DataFrames are available.")

print("\n### Summary of Impact of Multi-attribute Reweighing Bias Mitigation:")
print("- **Impact on overall performance:**")
print("  - Accuracy: The multi-attribute reweighed model's accuracy ({:.4f}) is compared to the baseline ({:.4f}).".format(accuracy_mitigated, accuracy))
print("  - Classification Report: Review the classification reports above for performance variations across classes for each model. The mitigated model might have slightly different precision, recall, and F1-scores for different classes compared to the baseline.")

print("\n- **Impact on fairness across protected attributes:**")

if fairness_df_baseline is not None and 'fairness_df_mitigated' in locals():
    for attr in protected_attribute_cols_eval:
        print(f"\n  - **{attr}:**")
        # Recalculate filtered dataframes and relevant metrics for the summary
        attr_counts_baseline = fairness_df_baseline[attr].value_counts()
        main_groups_baseline = attr_counts_baseline[attr_counts_baseline > 5].index.tolist()
        attr_counts_mitigated = fairness_df_mitigated[attr].value_counts()
        main_groups_mitigated = attr_counts_mitigated[attr_counts_mitigated > 5].index.tolist()
        common_groups = list(set(main_groups_baseline) & set(main_groups_mitigated))

        if len(common_groups) < 2:
            print(f"    - Not enough common main groups (>{5} samples) to summarize fairness changes for this attribute.")
            continue

        # Ensure indices are aligned before filtering
        baseline_df_filtered = fairness_df_baseline[fairness_df_baseline[attr].isin(common_groups)].copy()
        mitigated_df_filtered = fairness_df_mitigated[fairness_df_mitigated[attr].isin(common_groups)].copy()

        predicted_outcome_rates_baseline = baseline_df_filtered.groupby(attr)['Predicted_Target'].apply(lambda x: (x == favorable_outcome_label).mean())
        predicted_outcome_rates_mitigated = mitigated_df_filtered.groupby(attr)['Predicted_Target'].apply(lambda x: (x == favorable_outcome_label).mean())

        # Filter for instances where the true outcome is the favorable outcome for summary calculations
        favorable_outcome_df_baseline = baseline_df_filtered[baseline_df_filtered['True_Target'] == favorable_outcome_label].copy()
        favorable_outcome_df_mitigated = mitigated_df_filtered[mitigated_df_filtered['True_Target'] == favorable_outcome_label].copy()

        # Add numerical target values for confusion matrix calculation in summary
        if len(favorable_outcome_df_baseline) > 0:
            favorable_outcome_df_baseline['True_Target_Value'] = favorable_outcome_df_baseline['True_Target'].map(target_mapping)
            favorable_outcome_df_baseline['Predicted_Target_Value'] = favorable_outcome_df_baseline['Predicted_Target'].map(target_mapping)
            tpr_rates_baseline = favorable_outcome_df_baseline.groupby(attr)[['True_Target_Value', 'Predicted_Target_Value']].apply(
                lambda x: confusion_matrix(x['True_Target_Value'], x['Predicted_Target_Value'], labels=[0, 1, 2])[favorable_outcome_value, favorable_outcome_value] / (x['True_Target_Value'] == favorable_outcome_value).sum() if (x['True_Target_Value'] == favorable_outcome_value).sum() > 0 else 0
            )
        else:
            tpr_rates_baseline = None

        if len(favorable_outcome_df_mitigated) > 0:
            favorable_outcome_df_mitigated['True_Target_Value'] = favorable_outcome_df_mitigated['True_Target'].map(target_mapping)
            favorable_outcome_df_mitigated['Predicted_Target_Value'] = favorable_outcome_df_mitigated['Predicted_Target'].map(target_mapping)
            tpr_rates_mitigated = favorable_outcome_df_mitigated.groupby(attr)[['True_Target_Value', 'Predicted_Target_Value']].apply(
                lambda x: confusion_matrix(x['True_Target_Value'], x['Predicted_Target_Value'], labels=[0, 1, 2])[favorable_outcome_value, favorable_outcome_value] / (x['True_Target_Value'] == favorable_outcome_value).sum() if (x['True_Target_Value'] == favorable_outcome_value).sum() > 0 else 0
            )
        else:
            tpr_rates_mitigated = None


        # Summarize Disparate Impact change
        if baseline_metrics[attr]['di'] is not None and mitigated_metrics[attr]['di'] is not None:
             if not baseline_metrics[attr]['di'].empty and not mitigated_metrics[attr]['di'].empty:
                baseline_di_range = baseline_metrics[attr]['di'].max() - baseline_metrics[attr]['di'].min()
                mitigated_di_range = mitigated_metrics[attr]['di'].max() - mitigated_metrics[attr]['di'].min()
                print(f"    - Disparate Impact Range (closer to 0 is better): Baseline={baseline_di_range:.4f}, Mitigated={mitigated_di_range:.4f}. The range of Disparate Impact values across groups has likely decreased after mitigation.")
             else:
                 print(f"    - Disparate Impact: Not enough valid data points to summarize changes for this attribute.")
        else:
             print(f"    - Disparate Impact: Not enough valid data points to summarize changes for this attribute.")


        # Summarize Demographic Parity Difference change
        if baseline_metrics[attr]['dpd'] is not None and mitigated_metrics[attr]['dpd'] is not None:
            if not baseline_metrics[attr]['dpd'].empty and not mitigated_metrics[attr]['dpd'].empty:
                baseline_dpd_range = baseline_metrics[attr]['dpd'].max() - baseline_metrics[attr]['dpd'].min()
                mitigated_dpd_range = mitigated_metrics[attr]['dpd'].max() - mitigated_metrics[attr]['dpd'].min()
                print(f"    - Demographic Parity Difference Range (closer to 0 is better): Baseline={baseline_dpd_range:.4f}, Mitigated={mitigated_dpd_range:.4f}. The range of Demographic Parity Differences across groups has likely decreased after mitigation.")
            else:
                 print(f"    - Demographic Parity Difference: Not enough valid data points to summarize changes for this attribute.")
        else:
             print(f"    - Demographic Parity Difference: Not enough valid data points to summarize changes for this attribute.")


        # Summarize Equal Opportunity Difference change
        if baseline_metrics[attr]['eo_diff'] is not None and mitigated_metrics[attr]['eo_diff'] is not None:
            if not baseline_metrics[attr]['eo_diff'].empty and not mitigated_metrics[attr]['eo_diff'].empty:
                baseline_eo_range = baseline_metrics[attr]['eo_diff'].max() - baseline_metrics[attr]['eo_diff'].min()
                mitigated_eo_range = mitigated_metrics[attr]['eo_diff'].max() - mitigated_metrics[attr]['eo_diff'].min()
                print(f"    - Equal Opportunity Difference Range (closer to 0 is better): Baseline={baseline_eo_range:.4f}, Mitigated={mitigated_eo_range:.4f}. The range of Equal Opportunity Differences across groups has likely decreased after mitigation.")
            else:
                 print(f"    - Equal Opportunity Difference: Not enough valid data points with favorable outcomes to summarize changes for this attribute.")
        else:
             print(f"    - Equal Opportunity Difference: Not enough valid data points with favorable outcomes to summarize changes for this attribute.")


else:
    print("\nCould not summarize fairness changes. Ensure both baseline and mitigated fairness DataFrames are available.")


print("\n### Conclusion and Remaining Biases")
print("The multi-attribute reweighing technique aimed to reduce bias across several protected attributes simultaneously. Review the comparison of fairness metrics above to assess its effectiveness for each attribute. A smaller range for Disparate Impact and Demographic Parity Difference (closer to 0) and a smaller range for Equal Opportunity Difference (closer to 0) indicate reduced bias.")

print("\nRemaining Biases:")
print("Based on the mitigated model's fairness metrics, note any remaining disparities in predicted 'Graduate' outcomes across the protected attributes. Further mitigation or different techniques might be explored to address these.")

python -m pip install streamlit -q

python -m pip -q install pandas numpy scikit-learn joblib streamlit pyngrok reportlab
python -m sudo apt-get -q install -y nodejs npm   # for localtunnel (alternative to ngrok)

import joblib
joblib.dump(model, 'model.pkl')

model = joblib.load('model.pkl')

import matplotlib.pyplot as plt
import numpy as np

# Assuming baseline_metrics and mitigated_metrics dictionaries are available
# from the previous comparison step.
# Each dictionary has protected attributes as keys, and values are dictionaries
# containing 'di', 'dpd', and 'eo_diff' series.

# Define the metrics to plot
metrics_to_plot = ['di', 'dpd', 'eo_diff']
metric_titles = {
    'di': 'Disparate Impact Range',
    'dpd': 'Demographic Parity Difference Range',
    'eo_diff': 'Equal Opportunity Difference Range'
}

# Create a figure and a set of subplots
fig, axes = plt.subplots(nrows=len(metrics_to_plot), ncols=1, figsize=(10, 5 * len(metrics_to_plot)))

# Ensure axes is an array even if there's only one subplot
if len(metrics_to_plot) == 1:
    axes = [axes]

x = np.arange(len(protected_attribute_cols_eval)) # the label locations
width = 0.35 # the width of the bars

for i, metric in enumerate(metrics_to_plot):
    baseline_values = []
    mitigated_values = []
    labels = []

    for attr in protected_attribute_cols_eval:
        # Calculate the range for each metric for both baseline and mitigated models
        # Handle cases where a metric might be None or empty
        baseline_range = 0
        if baseline_metrics.get(attr) and baseline_metrics[attr].get(metric) is not None and not baseline_metrics[attr][metric].empty:
             baseline_range = baseline_metrics[attr][metric].max() - baseline_metrics[attr][metric].min()


        mitigated_range = 0
        if mitigated_metrics.get(attr) and mitigated_metrics[attr].get(metric) is not None and not mitigated_metrics[attr][metric].empty:
            mitigated_range = mitigated_metrics[attr][metric].max() - mitigated_metrics[attr][metric].min()


        baseline_values.append(baseline_range)
        mitigated_values.append(mitigated_range)
        labels.append(attr)

    rects1 = axes[i].bar(x - width/2, baseline_values, width, label='Baseline')
    rects2 = axes[i].bar(x + width/2, mitigated_values, width, label='Mitigated')

    # Add some text for labels, title and custom x-axis tick labels, etc.
    axes[i].set_ylabel('Range of Metric')
    axes[i].set_title(f'{metric_titles[metric]} by Protected Attribute')
    axes[i].set_xticks(x)
    axes[i].set_xticklabels(labels)
    axes[i].legend()

    # Add value labels on top of bars
    def autolabel(rects):
        """Attach a text label above each bar in *rects*, displaying its height."""
        for rect in rects:
            height = rect.get_height()
            axes[i].annotate(f'{height:.2f}',
                        xy=(rect.get_x() + rect.get_width() / 2, height),
                        xytext=(0, 3),  # 3 points vertical offset
                        textcoords="offset points",
                        ha='center', va='bottom')

    autolabel(rects1)
    autolabel(rects2)

fig.tight_layout()

# Save the figure to a file
plt.savefig('fairness_plot.png')

print("Fairness plot saved as fairness_plot.png")

import streamlit as st
import joblib
import pandas as pd

st.title(" Student Outcome Predictor  Fair ML Dashboard")

model = joblib.load("model.pkl")

# Input section
st.sidebar.header("Enter Student Details")
age = st.sidebar.slider("Age at Enrollment", 15, 40, 20)
gender = st.sidebar.selectbox("Gender", ["Male", "Female"])
debtor = st.sidebar.selectbox("Debtor Status", ["No", "Yes"])
scholar = st.sidebar.selectbox("Scholarship Holder", ["No", "Yes"])
marital = st.sidebar.selectbox("Marital Status", ["Single", "Married", "Divorced"])

# Make prediction
if st.button("Predict Outcome"):
    data = pd.DataFrame({
        'Age': [age],
        'Gender': [0 if gender=="Male" else 1],
        'Debtor': [1 if debtor=="Yes" else 0],
        'Scholarship': [1 if scholar=="Yes" else 0],
        'Marital_Status': [marital]
    })
    prediction = model.predict(data)[0]
    st.success(f"Predicted Outcome: {prediction}")

# Fairness visualization
st.header(" Fairness Indicators (Before vs After Reweighting)")
st.image("fairness_plot.png", caption="Disparate Impact & EOD Comparison")

# This script will:
# 1) Generate a Streamlit app (`app_streamlit.py`) that loads a saved model pipeline and shows fairness dashboards.
# 2) Create helper scripts to train/export a model from `data.csv` and compute fairness metrics/plots.
# 3) Produce a concise 4-page PDF report for Assignment-4 (based on the user's Assignment-2 and the new UI).

import json
import os
from textwrap import dedent

base = "/mnt/data"
os.makedirs(base, exist_ok=True)

# -------------------------------
# 1) Create Streamlit App
# -------------------------------
app_code = dedent(r'''
import streamlit as st
import pandas as pd
import numpy as np
import joblib
import json
import os
from pathlib import Path

st.set_page_config(page_title="Student Outcome Predictor  Fair ML Dashboard", layout="wide")

st.title(" Student Outcome Predictor  Fair ML Dashboard")
st.caption("Built as an extension of Assignment-2: connects to your trained model, shows predictions, performance, and fairness.")

MODEL_PATH = "model.pkl"
SCHEMA_PATH = "feature_schema.json"
FAIR_DIR = "fairness_outputs"

@st.cache_resource
def load_model():
    if os.path.exists(MODEL_PATH):
        return joblib.load(MODEL_PATH)
    else:
        return None

@st.cache_resource
def load_schema():
    if os.path.exists(SCHEMA_PATH):
        with open(SCHEMA_PATH, "r") as f:
            return json.load(f)
    return None

model = load_model()
schema = load_schema()

col_left, col_right = st.columns([1.2, 1])

with col_left:
    st.subheader(" Input Student Attributes")
    if schema is None:
        st.warning("`feature_schema.json` not found. Please run `python train_model.py --data data.csv` to export the model and schema first.")
    else:
        # Render dynamic widgets from schema
        user_input = {}
        for feat in schema["features"]:
            name = feat["name"]
            ftype = feat["type"]
            if ftype == "categorical":
                choices = feat.get("choices", [])
                default = choices[0] if choices else None
                val = st.selectbox(name, choices, index=0 if choices else None, key=f"input_{name}")
                user_input[name] = val
            elif ftype == "numeric":
                min_v = feat.get("min", 0)
                max_v = feat.get("max", 100)
                default = feat.get("default", min_v)
                val = st.number_input(name, value=float(default), min_value=float(min_v), max_value=float(max_v), key=f"input_{name}")
                user_input[name] = val
            else:
                val = st.text_input(name, key=f"input_{name}")
                user_input[name] = val

        if st.button(" Predict Outcome"):
            if model is None:
                st.error("Model file `model.pkl` not found. Please train/export your model first.")
            else:
                df = pd.DataFrame([user_input])
                try:
                    pred = model.predict(df)[0]
                    proba = None
                    if hasattr(model, "predict_proba"):
                        # If multi-class, show class probabilities
                        proba = model.predict_proba(df)[0]
                    st.success(f"**Predicted Outcome:** {pred}")
                    if proba is not None:
                        proba_df = pd.DataFrame(proba.reshape(1,-1), columns=getattr(model, "classes_", [f"class_{i}" for i in range(len(proba))]))
                        st.write("**Class probabilities:**")
                        st.dataframe(proba_df, use_container_width=True)
                except Exception as e:
                    st.error(f"Prediction failed. Check that your schema and model use the same feature names/order. Details: {e}")

with col_right:
    st.subheader(" About this Model")
    st.markdown("""
    - **Source**: Trained from Assignment-2 pipeline (Random Forest with preprocessing).
    - **Goal**: Predict *Graduate / Dropout / Enrolled*.
    - **Fairness**: Displays Disparate Impact (DI) & Equal Opportunity Difference (EOD) for protected attributes.
    - **Note**: Use ethically; predictions are *estimates*, not guarantees.
    """)
    if os.path.exists(FAIR_DIR):
        for img in ["di_gender.png", "di_age.png", "di_debtor.png", "eod_gender.png"]:
            p = Path(FAIR_DIR) / img
            if p.exists():
                st.image(str(p), caption=img.replace("_", " ").replace(".png", "").upper())
    else:
        st.info("No fairness plots found yet. After training, run `python train_model.py --data data.csv` to compute fairness and generate figures.")

st.divider()
st.subheader(" Performance Snapshot")
perf_json = Path(FAIR_DIR) / "performance.json"
if perf_json.exists():
    perf = json.loads(perf_json.read_text())
    st.json(perf)
else:
    st.caption("Performance metrics will appear here after training.")
''')

with open(os.path.join(base, "app_streamlit.py"), "w", encoding="utf-8") as f:
    f.write(app_code)

# -------------------------------
# 2) Training & Fairness Scripts
# -------------------------------

utils_code = dedent(r'''
import numpy as np
import pandas as pd

def infer_target_column(df: pd.DataFrame):
    # Try common labels
    candidates = [c for c in df.columns if c.lower() in ["target", "status", "outcome", "label", "class"]]
    if candidates:
        return candidates[0]
    # Heuristic: pick a low-cardinality (<=5) categorical column with values overlapping known classes
    known = {"graduate", "dropout", "enrolled"}
    for col in df.columns:
        if df[col].dtype == "object" or df[col].dtype.name == "category":
            uniq = set(map(lambda x: str(x).strip().lower(), df[col].dropna().unique().tolist()))
            if len(uniq) <= 5 and len(uniq & known) >= 2:
                return col
    # Fallback: None
    return None

def standard_feature_schema(df: pd.DataFrame):
    # Build a simple schema focusing on commonly used protected attributes + a few numerics if available
    schema = {"features": []}
    candidates = [
        ("Gender", "categorical"),
        ("Age at enrollment", "numeric"),
        ("Debtor", "categorical"),
        ("Scholarship holder", "categorical"),
        ("Marital status", "categorical"),
    ]
    # Use available columns; add basic min/max for numeric
    for name, ftype in candidates:
        # Try exact and case-insensitive match
        matches = [c for c in df.columns if c.strip().lower() == name.strip().lower()]
        if not matches:
            continue
        col = matches[0]
        if ftype == "categorical":
            cats = sorted(df[col].dropna().astype(str).unique().tolist())[:12]
            schema["features"].append({"name": col, "type": "categorical", "choices": cats})
        else:
            s = pd.to_numeric(df[col], errors="coerce")
            s = s.dropna()
            if s.empty:
                continue
            mn, mx = float(np.nanmin(s)), float(np.nanmax(s))
            default = float(np.nanmedian(s))
            schema["features"].append({"name": col, "type": "numeric", "min": mn, "max": mx, "default": default})
    # If nothing matched, fall back to first 6 columns with heuristics
    if not schema["features"]:
        for col in df.columns[:6]:
            if df[col].dtype == "object":
                cats = sorted(df[col].dropna().astype(str).unique().tolist())[:12]
                schema["features"].append({"name": col, "type": "categorical", "choices": cats})
            else:
                s = pd.to_numeric(df[col], errors="coerce").dropna()
                if s.empty:
                    continue
                mn, mx = float(np.nanmin(s)), float(np.nanmax(s))
                default = float(np.nanmedian(s))
                schema["features"].append({"name": col, "type": "numeric", "min": mn, "max": mx, "default": default})
    return schema

def compute_disparate_impact(y_true, y_pred, protected):
    # Favorable outcome is 'Graduate' if present; else the most frequent positive-like label
    labels = pd.Series(y_true).dropna().astype(str).str.lower().unique().tolist()
    fav = "graduate" if "graduate" in labels else (pd.Series(y_true).value_counts().idxmax())
    fav = str(fav).lower()
    df = pd.DataFrame({"y_true": y_true, "y_pred": y_pred, "g": protected})
    # Predicted positive rate per group
    di = {}
    rates = df.groupby("g")["y_pred"].apply(lambda s: np.mean(pd.Series(s).astype(str).str.lower()==fav))
    if rates.empty:
        return {}
    ref = rates.max() if rates.max() > 0 else 1.0
    for k, v in rates.items():
        di[k] = float(v / ref) if ref > 0 else 0.0
    return {"favorable": fav, "rates": rates.to_dict(), "di_vs_best": di}

def compute_eod(y_true, y_pred, protected):
    # EOD = TPR_group - TPR_ref, where positive is the favorable outcome
    yt = pd.Series(y_true).astype(str).str.lower()
    yp = pd.Series(y_pred).astype(str).str.lower()
    fav = "graduate" if "graduate" in yt.unique().tolist() else yt.value_counts().idxmax()
    df = pd.DataFrame({"yt": yt, "yp": yp, "g": protected})
    tpr = {}
    for g, grp in df.groupby("g"):
        pos = grp[grp["yt"]==fav]
        if len(pos)==0:
            tpr[g] = np.nan
        else:
            tpr[g] = float(np.mean(pos["yp"]==fav))
    # Reference = max TPR (closest to best-case)
    ref = np.nanmax(list(tpr.values()))
    eod = {k: (float(v - ref) if not np.isnan(v) else np.nan) for k, v in tpr.items()}
    return {"favorable": fav, "tpr": tpr, "eod_vs_best": eod}
''')

with open(os.path.join(base, "utils_fairness.py"), "w", encoding="utf-8") as f:
    f.write(utils_code)

train_code = dedent(r'''
import argparse
import json
import os
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder, StandardScaler

from utils_fairness import infer_target_column, standard_feature_schema, compute_disparate_impact, compute_eod

def main(data_path: str):
    df = pd.read_csv(data_path, sep=';') # Added sep=';' here
    # Explicitly set target column
    target_col = 'Target'

    if target_col not in df.columns:
         raise ValueError(f"Target column '{target_col}' not found in the dataset.")

    y = df[target_col].astype(str)
    X = df.drop(columns=[target_col])

    # Identify numeric and categorical columns
    num_cols = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]
    cat_cols = [c for c in X.columns if c not in num_cols]

    pre = ColumnTransformer([
        ("num", StandardScaler(), num_cols),
        ("cat", OneHotEncoder(handle_unknown="ignore"), cat_cols),
    ])

    clf = RandomForestClassifier(n_estimators=300, random_state=42, class_weight="balanced_subsample")
    pipe = Pipeline([("pre", pre), ("rf", clf)])

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

    pipe.fit(X_train, y_train)
    y_pred = pipe.predict(X_test)

    acc = accuracy_score(y_test, y_pred)
    macro_f1 = f1_score(y_test, y_pred, average="macro")

    print(f"Accuracy: {acc:.4f}")
    print(f"Macro F1: {macro_f1:.4f}")
    print(classification_report(y_test, y_pred))

    # Save model
    joblib.dump(pipe, "model.pkl")
    # Save schema for UI
    schema = standard_feature_schema(df.drop(columns=[target_col]))
    with open("feature_schema.json", "w") as f:
        json.dump(schema, f, indent=2)

    # Compute basic fairness metrics for common protected attributes if present
    fair_dir = Path("fairness_outputs")
    fair_dir.mkdir(exist_ok=True)

    perf = {"accuracy": float(acc), "macro_f1": float(macro_f1)}
    (fair_dir / "performance.json").write_text(json.dumps(perf, indent=2))

    import matplotlib.pyplot as plt

    def safe_plot_di(attr_name, filename):
        cols = [c for c in df.columns if c.strip().lower() == attr_name.strip().lower()]
        if not cols:
            return
        g = df.loc[y_test.index, cols[0]]
        di = compute_disparate_impact(y_test, y_pred, g)
        if not di:
            return
        rates = di["rates"]
        keys = list(rates.keys())
        vals = [rates[k] for k in keys]
        plt.figure()
        plt.bar([str(k) for k in keys], vals)
        plt.title(f"Predicted Positive Rate by {attr_name}")
        plt.xlabel(attr_name)
        plt.ylabel("Predicted Graduate Rate")
        plt.xticks(rotation=30, ha="right")
        plt.tight_layout()
        plt.savefig(fair_dir / filename)
        plt.close()

    def safe_plot_eod(attr_name, filename):
        cols = [c for c in df.columns if c.strip().lower() == attr_name.strip().lower()]
        if not cols:
            return
        g = df.loc[y_test.index, cols[0]]
        eod = compute_eod(y_test, y_pred, g)
        tpr = eod["tpr"]
        keys = list(tpr.keys())
        vals = [tpr[k] if tpr[k] is not None else np.nan for k in keys]
        plt.figure()
        plt.bar([str(k) for k in keys], vals)
        plt.title(f"TPR by {attr_name} (favored='{eod['favorable']}')")
        plt.xlabel(attr_name)
        plt.ylabel("TPR")
        plt.xticks(rotation=30, ha="right")
        plt.tight_layout()
        plt.savefig(fair_dir / filename)
        plt.close()

    # Try common protected attributes
    safe_plot_di("Gender", "di_gender.png")
    safe_plot_di("Age at enrollment", "di_age.png")
    safe_plot_di("Debtor", "di_debtor.png")
    safe_plot_eod("Gender", "eod_gender.png")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--data", type=str, default="data.csv", help="Path to CSV (e.g., student dropout dataset).")
    args = parser.parse_args()
    main(args.data)
''')

with open(os.path.join(base, "train_model.py"), "w", encoding="utf-8") as f:
    f.write(train_code)

# -------------------------------
# 3) Generate the Assignment-4 Report (PDF)
# -------------------------------
from reportlab.lib.pagesizes import A4
from reportlab.lib.styles import getSampleStyleSheet
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle, PageBreak, ListFlowable, ListItem
from reportlab.lib import colors

report_path = os.path.join(base, "Assignment4_Report.pdf")

styles = getSampleStyleSheet()
story = []

title = Paragraph("<b>CS698Y  Assignment 4: User Interface for ML Models (Extension of A2)</b>", styles["Title"])
story.append(title)
story.append(Spacer(1, 12))

intro = Paragraph(
    "This report extends our Assignment-2 work on fairness-aware prediction for student outcomes "
    "(Graduate, Dropout, Enrolled). We implemented a web interface that connects to the trained "
    "model, enables responsible predictions, and surfaces fairness indicators. We also evaluated the UI "
    "against Microsofts HAX principles.", styles["BodyText"])
story.append(intro)
story.append(Spacer(1, 12))

# Section 1: User & Interface
story.append(Paragraph("<b>1. Target User and Interface Design</b>", styles["Heading2"]))
bullets = ListFlowable([
    ListItem(Paragraph("Primary users: academic administrators/program coordinators who need early risk flags and cohort insights.", styles["BodyText"])),
    ListItem(Paragraph("Framework: Streamlit front-end over a scikit-learn pipeline exported from Assignment-2.", styles["BodyText"])),
    ListItem(Paragraph("Features: input widgets for student attributes; instant predictions with class probabilities; performance & fairness dashboards.", styles["BodyText"])),
    ListItem(Paragraph("Responsible AI: scope/limitations, fairness metrics (Disparate Impact, Equal Opportunity Difference), and plain-language guidance.", styles["BodyText"])),
], bulletType='bullet')
story.append(bullets)
story.append(Spacer(1, 12))

# Section 2: Backend Connection
story.append(Paragraph("<b>2. Backend Model Connection</b>", styles["Heading2"]))
story.append(Paragraph(
    "We export the trained Random Forest pipeline as <i>model.pkl</i> and a <i>feature_schema.json</i> (generated by <i>train_model.py</i>). "
    "The Streamlit app (<i>app_streamlit.py</i>) loads both; the schema drives UI widgets to ensure feature names and encodings align.", styles["BodyText"]))
story.append(Spacer(1, 12))

# Section 3: Screens (described)
story.append(Paragraph("<b>3. Screens & Workflows</b>", styles["Heading2"]))
screen_list = ListFlowable([
    ListItem(Paragraph("<b>Dashboard</b>: quick overview; performance metrics; fairness plots for protected attributes.", styles["BodyText"])),
    ListItem(Paragraph("<b>Prediction</b>: enter attributes (e.g., Gender, Age at enrollment, Debtor, Scholarship holder, Marital status)  predicted outcome + probabilities.", styles["BodyText"])),
    ListItem(Paragraph("<b>About</b>: capabilities, limitations, fairness notes, and links to code.", styles["BodyText"])),
], bulletType='bullet')
story.append(screen_list)
story.append(Spacer(1, 12))

# Section 4: Fairness Communication
story.append(Paragraph("<b>4. Fairness Indicators</b>", styles["Heading2"]))
story.append(Paragraph(
    "We compute and visualize Disparate Impact (DI) and Equal Opportunity Difference (EOD) for protected attributes present in the dataset, "
    "notably Gender, Age at enrollment, Debtor, Scholarship holder, and Marital status (if available). "
    "The interface shows predicted positive rates and TPRs per subgroup. Favorable outcome is treated as 'Graduate' when present. "
    "This mirrors Assignment-2s emphasis on subgroup equity.", styles["BodyText"]))
story.append(Spacer(1, 12))

# Section 5: HAX Evaluation Table
story.append(Paragraph("<b>5. HAX Principles Evaluation (Summary)</b>", styles["Heading2"]))
data = [
    ["Guideline", "Status", "Notes"],
    ["1. Make clear what the system can do", "Met", "Dashboard/About describe capabilities & scope."],
    ["2. Make clear how well it can do", "Met", "Accuracy/Macro-F1 shown; fairness plots included."],
    ["3. Time services based on context", "Partially Met", "On-demand predictions; cohort stats reflect current data."],
    ["4. Show contextually relevant info", "Met", "UI focuses on fields admins use; minimal clutter."],
    ["5. Match relevant social norms", "Met", "Neutral tone; responsible use guidance."],
    ["6. Mitigate social biases", "Met", "Fairness metrics displayed and discussed."],
    ["7. Support efficient invocation", "Met", "Simple controls; one-click prediction."],
    ["8. Support efficient dismissal", "Met", "Clear navigation & resettable inputs."],
    ["9. Support efficient correction", "Partially Met", "Users can re-run with changed inputs; no inline edit history."],
    ["10. Scope services when in doubt", "Met", "Validation and friendly error messages."],
    ["11. Make clear why it did what it did", "Met", "Model overview; feature schema transparency."],
    ["12. Remember recent interactions", "Not Met", "No session memory (can be added)."],
    ["13. Learn from user behavior", "Not Met", "Static model; retraining offline only."],
    ["14. Update/adapt cautiously", "N/A", "No online learning."],
    ["15. Encourage granular feedback", "Partially Met", "Could add in-app feedback form."],
    ["16. Convey consequences of actions", "Met", "Explains uncertainty and non-deterministic nature."],
    ["17. Provide global controls", "Partially Met", "Basic navigation; could add user settings."],
    ["18. Notify users about changes", "N/A", "Static deployment; release notes recommended."],
]
table = Table(data, colWidths=[210, 90, 240])
table.setStyle(TableStyle([
    ('BACKGROUND', (0,0), (-1,0), colors.lightgrey),
    ('GRID', (0,0), (-1,-1), 0.5, colors.grey),
    ('VALIGN', (0,0), (-1,-1), 'TOP'),
]))
story.append(table)
story.append(Spacer(1, 12))

# Section 6: How to Run
story.append(Paragraph("<b>6. How to Run</b>", styles["Heading2"]))
story.append(Paragraph(
    "1) Place <i>data.csv</i> in the same folder.  "
    "2) Train/export: <code>python train_model.py --data data.csv</code>  creates <i>model.pkl</i>, <i>feature_schema.json</i>, and fairness plots.  "
    "3) Launch UI: <code>streamlit run app_streamlit.py</code>.  "
    "4) Optional: Deploy on Streamlit Community Cloud.", styles["BodyText"]))
story.append(Spacer(1, 12))

# Section 7: Limitations
story.append(Paragraph("<b>7. Limitations & Future Work</b>", styles["Heading2"]))
story.append(Paragraph(
    "We currently compute fairness on the test split and display static plots. Future work: "
    "session memory, confidence intervals, feedback channel, and role-based access.", styles["BodyText"]))

story.append(PageBreak())

# Add a final page with placeholders for screenshots (users will replace with actual app screenshots)
story.append(Paragraph("<b>Appendix: Paste UI Screenshots</b>", styles["Heading2"]))
story.append(Paragraph("Add screenshots of Dashboard, Prediction, and About sections here after launching the app.", styles["BodyText"]))

doc = SimpleDocTemplate(report_path, pagesize=A4)
doc.build(story)

report_path

python /mnt/data/train_model.py --data data.csv

from pyngrok import ngrok
from google.colab import userdata

# Load the authtoken from Colab secrets
NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')

# Authenticate ngrok
ngrok.set_auth_token(NGROK_AUTH_TOKEN)

print("ngrok authtoken configured.")

python -m streamlit run /mnt/data/app_streamlit.py

import threading, time, subprocess
import os

def run_streamlit():
    # Use 0.0.0.0 to make the app accessible externally
    subprocess.call(["streamlit","run","app_streamlit.py","--server.port","8501","--server.address","0.0.0.0","--server.headless","true"])

# Kill any existing Streamlit processes
os.system("pkill streamlit")

threading.Thread(target=run_streamlit, daemon=True).start()
time.sleep(5)  # let Streamlit boot

# Start a tunnel using ngrok
# You might need to install ngrok first if it's not available
# !pip install pyngrok -q
from pyngrok import ngrok

# Kill any existing ngrok tunnels
ngrok.kill()

# Connect to the streamlit port
public_url = ngrok.connect(8501, bind_tls=True)

print(f"Your Streamlit app is running at: {public_url}")